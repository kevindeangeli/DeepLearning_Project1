{\rtf1\ansi\ansicpg1252\cocoartf2511
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Oblique;\f1\fswiss\fcharset0 Helvetica;\f2\fswiss\fcharset0 Helvetica-BoldOblique;
}
{\colortbl;\red255\green255\blue255;\red81\green136\blue67;\red32\green32\blue32;\red191\green100\blue38;
\red153\green168\blue186;\red109\green109\blue109;\red160\green0\blue163;\red128\green63\blue122;\red88\green118\blue71;
\red86\green132\blue173;\red117\green114\blue185;\red254\green187\blue91;\red152\green54\blue29;}
{\*\expandedcolortbl;;\csgenericrgb\c31765\c53333\c26275;\csgenericrgb\c12549\c12549\c12549;\csgenericrgb\c74902\c39216\c14902;
\csgenericrgb\c60000\c65882\c72941;\csgenericrgb\c42745\c42745\c42745;\csgenericrgb\c62745\c0\c63922;\csgenericrgb\c50196\c24706\c47843;\csgenericrgb\c34510\c46275\c27843;
\csgenericrgb\c33725\c51765\c67843;\csgenericrgb\c45882\c44706\c72549;\csgenericrgb\c99608\c73333\c35686;\csgenericrgb\c59608\c21176\c11373;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\i\fs38 \cf2 \cb3 '''\
Created by: Kevin De Angeli\
Email: kevindeangeli@utk.edu\
Date: 2020-01-15\
'''\
\
\

\f1\i0 \cf4 from \cf5 errorFunctions \cf4 import \cf5 * \cf6 #I put the error functions and their derivatives in a different file.\
\cf4 from \cf5 activation \cf4 import \cf5 *     \cf6 #I put the activation functions and their derivatives in a different file.\
\cf4 import \cf5 numpy \cf4 as \cf5 np\
\cf4 import \cf5 sys\
\cf4 import \cf5 matplotlib.pyplot \cf4 as \cf5 plt\
\
\
\
\cf4 class \cf5 Neuron():\
\
    \cf4 def \cf7 __init__\cf5 (\cf8 self\cf4 ,\cf5 inputLen\cf4 , \cf5 activationFun = \cf9 "sigmoid"\cf4 , \cf5 learningRate = \cf10 .5\cf4 , \cf5 weights = \cf4 None, \cf5 bias = \cf4 None\cf5 ):\
        \cf8 self\cf5 .inputLen = inputLen\
        \cf8 self\cf5 .learnR = learningRate\
        \cf8 self\cf5 .activationFunction = activationFun\
\
        \cf8 self\cf5 .output = \cf4 None \cf6 #for backprop.\
        \cf8 self\cf5 .input = \cf4 None   \cf6 #saves the input to the neuron for backprop.\
        \cf8 self\cf5 .newWeights = [] \cf6 #saves new weight but it doesn't update until the end of backprop. (method: updateWeight)\
        \cf8 self\cf5 .delta = \cf4 None\
\
        if \cf5 weights \cf4 is None\cf5 :\
            \cf6 #set them to random:\
            \cf8 self\cf5 .weights = [np.random.random_sample() \cf4 for \cf6 i \cf4 in \cf11 range\cf5 (inputLen)]\
            \cf8 self\cf5 .bias = np.random.random_sample()\
\
        \cf4 else\cf5 :\
            \cf8 self\cf5 .weights = weights\
            \cf8 self\cf5 .bias = bias\
\
\
\
    \cf4 def \cf12 updateWeight\cf5 (\cf8 self\cf5 ):\
        \cf8 self\cf5 .weights = \cf8 self\cf5 . newWeights\
        \cf8 self\cf5 .newWeights = []\
\
\
    \cf4 def \cf12 activate\cf5 (\cf8 self\cf4 ,\cf5 x):\
        
\f0\i \cf2 '''\
        given a value returns its value after activation(depending on the activation function used).\
        
\f2\b :return
\f0\b0 :\
        '''\
        
\f1\i0 \cf4 if \cf8 self\cf5 .activationFunction == \cf9 "sigmoid"\cf5 :\
           \cf4 return \cf5 sigmoid(x)\
        \cf6 #else:\
        #    return linear(x)\
\
\
    \cf4 def \cf12 calculate\cf5 (\cf8 self\cf4 , \cf5 input):\
        
\f0\i \cf2 '''\
        Given an input, it will calculate the output\
        
\f2\b :return
\f0\b0 :\
        '''\
        
\f1\i0 \cf8 self\cf5 .input = input\
        \cf8 self\cf5 .output = \cf8 self\cf5 .activate(np.dot(input\cf4 ,\cf8 self\cf5 .weights) + \cf8 self\cf5 .bias)\
        \cf4 return \cf8 self\cf5 .output\
\
\
    \cf4 def \cf12 backpropagationLastLayer\cf5 (\cf8 self\cf4 , \cf5 target):\
        \cf8 self\cf5 .delta = mse_prime(\cf8 self\cf5 .output\cf4 , \cf5 target) * sigmoid_prime(\cf8 self\cf5 .output)\
        \cf4 for \cf5 index\cf4 , \cf5 PreviousNeuronOutput \cf4 in \cf11 enumerate\cf5 (\cf8 self\cf5 .input):\
            \cf8 self\cf5 .newWeights.append(\cf8 self\cf5 .weights[index] - \cf8 self\cf5 .learnR * \cf8 self\cf5 .delta * PreviousNeuronOutput)\
\
    \cf4 def \cf12 backpropagation\cf5 (\cf8 self\cf4 , \cf5 sumDelta):\
        \cf6 #sumDelta will be computed at the layer level. Since it requires weights from multiple neurons.\
        \cf8 self\cf5 .delta = sumDelta * sigmoid_prime(\cf8 self\cf5 .output)\
        \cf4 for \cf5 index\cf4 , \cf5 PreviousNeuronOutput \cf4 in \cf11 enumerate\cf5 (\cf8 self\cf5 .input):\
            \cf8 self\cf5 .newWeights.append(\cf8 self\cf5 .weights[index] - \cf8 self\cf5 .learnR * \cf8 self\cf5 .delta * \cf8 self\cf5 .input[index])\
\
    \cf4 def \cf12 mini_Delta\cf5 (\cf8 self\cf4 , \cf5 index):\
        \cf4 return \cf8 self\cf5 .delta * \cf8 self\cf5 .weights[index]\
\
\
\
\
\
\cf4 class \cf5 FullyConnectedLayer():\
    \cf4 def \cf7 __init__\cf5 (\cf8 self\cf4 , \cf5 inputLen\cf4 , \cf5 numOfNeurons = \cf10 5\cf4 , \cf5 activationFun = \cf9 "sigmoid"\cf4 , \cf5 learningRate = \cf10 .1\cf4 , \cf5 weights = \cf4 None, \cf5 bias = \cf4 None\cf5 ):\
        \cf8 self\cf5 .inputLen = inputLen\
        \cf8 self\cf5 .neuronsNum = numOfNeurons\
        \cf8 self\cf5 .activationFun = activationFun\
        \cf8 self\cf5 .learningRate = learningRate\
        \cf8 self\cf5 .weights = weights\
        \cf8 self\cf5 .bias = bias\
        \cf8 self\cf5 .layerOutput = []\
\
        \cf4 if \cf5 weights \cf4 is None\cf5 :\
            \cf8 self\cf5 .neurons = [Neuron(\cf13 inputLen\cf5 =\cf8 self\cf5 .inputLen\cf4 , \cf13 activationFun\cf5 =activationFun\cf4 , \cf13 learningRate\cf5 =\cf8 self\cf5 .learningRate\cf4 , \cf13 weights\cf5 =\cf8 self\cf5 .weights) \cf4 for \cf6 i \cf4 in \cf11 range\cf5 (numOfNeurons)]\
        \cf4 else\cf5 :\
            \cf8 self\cf5 .neurons = [Neuron(\cf13 inputLen\cf5 =\cf8 self\cf5 .inputLen\cf4 , \cf13 activationFun\cf5 =activationFun\cf4 , \cf13 learningRate\cf5 =\cf8 self\cf5 .learningRate\cf4 , \cf13 weights\cf5 =\cf8 self\cf5 .weights[i]\cf4 , \cf13 bias\cf5 = \cf8 self\cf5 .bias[i]) \cf4 for \cf5 i \cf4 in \cf11 range\cf5 (numOfNeurons)]\
\
\
    \cf4 def \cf12 calculate\cf5 (\cf8 self\cf4 , \cf5 input):\
        
\f0\i \cf2 '''\
        Will calculate the output of all the neurons in the layer.\
        
\f2\b :return
\f0\b0 :\
        '''\
        
\f1\i0 \cf8 self\cf5 .layerOutput = []\
        \cf4 for \cf5 neuron \cf4 in \cf8 self\cf5 .neurons:\
            \cf8 self\cf5 .layerOutput.append(neuron.calculate(input))\
\
        \cf4 return \cf8 self\cf5 .layerOutput\
\
    \cf4 def \cf12 backPropagateLast\cf5 (\cf8 self\cf4 , \cf5 target):\
        \cf4 for \cf5 targetIndex\cf4 , \cf5 neuron \cf4 in \cf11 enumerate\cf5 (\cf8 self\cf5 .neurons):\
            \cf6 #neuron.update_weight(   target[targetIndex], self.layerOutput)\
            \cf5 neuron.backpropagationLastLayer(\cf13 target\cf5 =target[targetIndex])\
\
    \cf4 def \cf12 updateWeights\cf5 (\cf8 self\cf5 ):\
        \cf4 for \cf5 neuron \cf4 in \cf8 self\cf5 .neurons:\
            neuron.updateWeight()\
\
    \cf4 def \cf12 deltaSum\cf5 (\cf8 self\cf5 ):\
        delta_sum  = \cf10 0\
        \cf4 for \cf5 index\cf4 , \cf5 neuron \cf4 in \cf11 enumerate\cf5 (\cf8 self\cf5 .neurons):\
            delta_sum += neuron.mini_Delta(index)\
        \cf4 return \cf5 delta_sum\
\
    \cf4 def \cf12 backpropagation\cf5 (\cf8 self\cf4 , \cf5 delta):\
        \cf4 for \cf5 neuron \cf4 in \cf8 self\cf5 .neurons:\
            neuron.backpropagation(delta)\
\
\
\
\cf4 class \cf5 NeuralNetwork():\
    \cf4 def \cf7 __init__\cf5 (\cf8 self\cf4 , \cf5 neuronsNum = \cf4 None, \cf5 activationVector = \cf10 0\cf4 , \cf5 lossFunction = \cf9 "MSE"\cf4 , \cf5 learningRate = \cf10 .1\cf4 , \cf5 weights = \cf4 None, \cf5 bias = \cf4 None\cf5 ):\
        \cf8 self\cf5 .inputLen   = neuronsNum[\cf10 0\cf5 ]\
        \cf8 self\cf5 .layersNum  = \cf11 len\cf5 (neuronsNum)-\cf10 1 \cf6 #Don't count the first one (input).\
        \cf8 self\cf5 .activationVector = activationVector\
        \cf8 self\cf5 .lossFunction = lossFunction\
        \cf8 self\cf5 .learningRate = learningRate\
        \cf8 self\cf5 .weights = weights\
        \cf8 self\cf5 .bias = bias\
\
        \cf4 if \cf5 neuronsNum \cf4 is None \cf5 :  \cf6 #By default, each layer will have 5 neurons, unless specified.\
            \cf8 self\cf5 .neuronsNum = [\cf10 3 \cf4 for \cf6 i \cf4 in \cf11 range\cf5 (layersNum)]\
        \cf4 else\cf5 :\
            \cf8 self\cf5 .neuronsNum = neuronsNum[\cf10 1\cf5 :\cf11 len\cf5 (neuronsNum)] \cf6 #Don't count he first one.\
\
        \cf4 if \cf5 activationVector \cf4 is None or \cf5 activationVector != \cf8 self\cf5 .layersNum: \cf6 #This is the default vector if a problem is encountered or the vector is not provided when the class is created.\
            \cf8 self\cf5 .activationVector = [\cf9 "sigmoid" \cf4 for \cf6 i \cf4 in \cf11 range\cf5 (\cf8 self\cf5 .layersNum)]\
\
        \cf6 #Define the layers of the networks with the respective neurons:\
        \cf8 self\cf5 .layers = []\
        inputLenLayer = \cf8 self\cf5 .inputLen\
\
        \cf4 if \cf5 weights \cf4 is None\cf5 :\
            \cf4 for \cf5 i \cf4 in \cf11 range\cf5 (\cf8 self\cf5 .layersNum):\
                \cf8 self\cf5 .layers.append(\
                    FullyConnectedLayer(\cf13 numOfNeurons\cf5 =\cf8 self\cf5 .neuronsNum[i]\cf4 , \cf13 activationFun\cf5 =\cf8 self\cf5 .activationVector[i]\cf4 , \cf13 inputLen\cf5 =inputLenLayer\cf4 , \cf13 learningRate\cf5 =\cf8 self\cf5 .learningRate\cf4 , \cf13 weights\cf5 =\cf8 self\cf5 .weights))\
                \cf6 # The number of weights in one layer depends on the number of neurons in the previous layer:\
                \cf5 inputLenLayer = \cf8 self\cf5 .neuronsNum[i]\
\
\
        \cf4 else\cf5 :\
            \cf4 for \cf5 i \cf4 in \cf11 range\cf5 (\cf8 self\cf5 .layersNum):\
                \cf8 self\cf5 .layers.append(\
                    FullyConnectedLayer(\cf13 numOfNeurons\cf5 =\cf8 self\cf5 .neuronsNum[i]\cf4 , \cf13 activationFun\cf5 =\cf8 self\cf5 .activationVector[i]\cf4 , \cf13 inputLen\cf5 =inputLenLayer\cf4 , \cf13 learningRate\cf5 =\cf8 self\cf5 .learningRate\cf4 , \cf13 weights\cf5 =\cf8 self\cf5 .weights[i]\cf4 , \cf13 bias\cf5 =\cf8 self\cf5 .bias[i]))\
                \cf6 # The number of weights in one layer depends on the number of neurons in the previous layer:\
                \cf5 inputLenLayer = \cf8 self\cf5 .neuronsNum[i]\
\
\
\
    \cf4 def \cf12 showWeights\cf5 (\cf8 self\cf5 ):\
        \cf6 #Function which just goes through each neuron in each layer and displays the weights.\
        inputLenLayer \cf5 = \cf8 self\cf5 .inputLen\
        \cf4 for \cf5 i \cf4 in \cf11 range\cf5 (\cf8 self\cf5 .layersNum):\
            \cf11 print\cf5 (\cf9 " "\cf5 )\
            \cf4 for \cf5 k \cf4 in \cf11 range\cf5 (\cf8 self\cf5 .neuronsNum[i]):\
                \cf11 print\cf5 (\cf8 self\cf5 .layers[i].neurons[k].weights)\
\
            \cf6 inputLenLayer \cf5 = \cf8 self\cf5 .neuronsNum[i]\
\
    \cf4 def \cf12 showBias\cf5 (\cf8 self\cf5 ):\
        \cf6 #Function which just goes through each neuron in each layer and displays the weights.\
        inputLenLayer \cf5 = \cf8 self\cf5 .inputLen\
        \cf4 for \cf5 i \cf4 in \cf11 range\cf5 (\cf8 self\cf5 .layersNum):\
            \cf11 print\cf5 (\cf9 " "\cf5 )\
            \cf4 for \cf5 k \cf4 in \cf11 range\cf5 (\cf8 self\cf5 .neuronsNum[i]):\
                \cf11 print\cf5 (\cf8 self\cf5 .layers[i].neurons[k].bias)\
\
            \cf6 inputLenLayer \cf5 = \cf8 self\cf5 .neuronsNum[i]\
\
\
\
    \cf4 def \cf12 calculate\cf5 (\cf8 self\cf4 , \cf5 input):\
        
\f0\i \cf2 '''\
        given an input calculates the output of the network.\
        input should be a list.\
        
\f2\b :return
\f0\b0 :\
        '''\
        
\f1\i0 \cf5 output = input\
        \cf4 for \cf5 layer \cf4 in \cf8 self\cf5 .layers:\
            output = layer.calculate(output)\
\
        \cf4 return \cf5 output\
\
    \cf4 def \cf12 backPropagate\cf5 (\cf8 self\cf4 , \cf5 target):\
        \cf8 self\cf5 .layers[-\cf10 1\cf5 ].backPropagateLast(target)\
        layersCounter = \cf8 self\cf5 .layersNum+\cf10 1\
\
        \cf4 for \cf5 i \cf4 in \cf11 range\cf5 (\cf10 2\cf4 ,\cf5 layersCounter):\
            delta = \cf8 self\cf5 .layers[-i + \cf10 1\cf5 ].deltaSum()\
            \cf8 self\cf5 .layers[-i].backpropagation(delta)\
\
        \cf4 for \cf5 layer \cf4 in \cf8 self\cf5 .layers:\
            layer.updateWeights()\
\
\
\
    \cf4 def \cf12 calculateLoss\cf5 (\cf8 self\cf4 ,\cf5 input\cf4 ,\cf5 desired_output\cf4 , \cf5 function = \cf9 "MSE"\cf5 ):\
        
\f0\i \cf2 '''\
        Given an input and desired output, calculate the loss.\
        Can be implemented with MSE and binary cross.\
        
\f2\b :param
\f0\b0  input:\
        
\f2\b :param
\f0\b0  output:\
        
\f2\b :return
\f0\b0 :\
        '''\
        
\f1\i0 \cf6 N \cf5 = \cf11 len\cf5 (input)\
        output = \cf8 self\cf5 .calculate(input)\
        \cf4 if \cf5 function == \cf9 "MSE"\cf5 :\
            error = mse(output\cf4 , \cf5 desired_output)\
        \cf4 else\cf5 :\
            binary_cross_entropy_loss(output\cf4 , \cf5 desired_output)\
\
        \cf4 return \cf5 error\
\
\
\
    \cf4 def \cf12 train\cf5 (\cf8 self\cf4 , \cf5 input\cf4 , \cf5 target):\
        
\f0\i \cf2 '''\
        Basically, do forward and backpropagation all together here.\
        Given a single input and desired output, it will take one step of gradient descent.\
        
\f2\b :return
\f0\b0 :\
        '''\
        
\f1\i0 \cf8 self\cf5 .calculate(input)\
        \cf8 self\cf5 .backPropagate(target)\
\
\
\
\cf4 def \cf12 doExample\cf5 ():\
    \cf11 print\cf5 ( \cf9 "--- Example ---"\cf5 )\
\
    \cf6 #Let's try the class example by setting the bias and weights:\
    \cf5 Newweights = [[[\cf10 .15\cf4 ,\cf10 .20\cf5 ]\cf4 , \cf5 [\cf10 .25\cf4 , \cf10 .30\cf5 ]]\cf4 , \cf5 [[\cf10 .40\cf4 , \cf10 .45\cf5 ]\cf4 , \cf5 [\cf10 .5\cf4 , \cf10 .55\cf5 ]]]\
    newBias = [[\cf10 .35\cf4 ,\cf10 .35\cf5 ]\cf4 ,\cf5 [\cf10 .6\cf4 ,\cf10 .6\cf5 ]]\
    model = NeuralNetwork(\cf13 neuronsNum\cf5 =[\cf10 2\cf4 , \cf10 2\cf4 , \cf10 2\cf5 ]\cf4 , \cf13 activationVector\cf5 =[\cf9 'sigmoid'\cf4 , \cf9 'sigmoid'\cf5 ]\cf4 , \cf13 lossFunction\cf5 =\cf9 "MSE"\cf4 ,\
                          \cf13 learningRate\cf5 =\cf10 .5\cf4 , \cf13 weights\cf5 =Newweights\cf4 , \cf13 bias \cf5 = newBias)\
\
\
\
    \cf11 print\cf5 (\cf9 "Original weights and biases of the network: "\cf5 )\
    model.showWeights()\
    model.showBias()\
\
\
    \cf11 print\cf5 (\cf9 "\cf4 \\n\cf9 Forward pass: "\cf5 )\
    \cf11 print\cf5 (model.calculate([\cf10 .05\cf4 ,\cf10 .1\cf5 ]))\
\
    \cf6 #model.train(input= [.05,.1], target=[.01, .99]) #you could use just this function to do all at once.\
    \cf5 model.backPropagate(\cf13 target\cf5 = [\cf10 .01\cf4 , \cf10 .99\cf5 ])\
    \cf11 print\cf5 (\cf9 "\cf4 \\n\cf9 After BackProp, the updated weights are:"\cf5 )\
    model.showWeights()\
\
\cf4 def \cf12 doAnd\cf5 ():\
    \cf11 print\cf5 ( \cf9 "--- AND ---"\cf5 )\
    x = [[\cf10 1\cf4 ,\cf10 1\cf5 ]\cf4 ,\cf5 [\cf10 1\cf4 ,\cf10 0\cf5 ]\cf4 ,\cf5 [\cf10 0\cf4 ,\cf10 1\cf5 ]\cf4 , \cf5 [\cf10 0\cf4 ,\cf10 0\cf5 ]]\
    y = [[\cf10 1\cf5 ]\cf4 ,\cf5 [\cf10 0\cf5 ]\cf4 , \cf5 [\cf10 0\cf5 ]\cf4 , \cf5 [\cf10 0\cf5 ]]\
\
    model = NeuralNetwork(\cf13 neuronsNum\cf5 =[\cf10 2\cf4 , \cf10 1\cf4 , \cf10 1\cf5 ]\cf4 , \cf13 activationVector\cf5 =[\cf9 'sigmoid'\cf5 ]\cf4 , \cf13 lossFunction\cf5 =\cf9 "MSE"\cf4 ,\
                          \cf13 learningRate\cf5 =\cf10 6\cf4 , \cf13 weights\cf5 =\cf4 None, \cf13 bias\cf5 =\cf4 None\cf5 )\
\
\
    \cf11 print\cf5 (\cf9 "-------- Before training ---------"\cf5 )\
    \cf11 print\cf5 (\cf9 "Model's Weights:"\cf5 )\
    model.showWeights()\
    \cf11 print\cf5 (\cf9 "Model's Bias:"\cf5 )\
    model.showBias()\
\
    \cf11 print\cf5 (\cf9 "-------- After training ---------"\cf5 )\
\
\
    \cf4 for \cf5 i \cf4 in \cf11 range\cf5 (\cf10 10000\cf5 ):\
        \cf4 for \cf5 index \cf4 in \cf11 range\cf5 (\cf11 len\cf5 (x)):\
            model.train(\cf13 input\cf5 =x[index]\cf4 ,\cf13 target\cf5 =y[index])\
\
    \cf11 print\cf5 (\cf9 "Predictions: "\cf5 )\
    \cf4 for \cf5 index2 \cf4 in \cf11 range\cf5 (\cf11 len\cf5 (x)):\
        \cf11 print\cf5 (\cf9 "\cf4 \\n\cf9 Predict: "\cf4 , \cf5 x[index2])\
        \cf11 print\cf5 (model.calculate(x[index2]))\
\
    \cf11 print\cf5 (\cf9 "Model's Weights:"\cf5 )\
    model.showWeights()\
    \cf11 print\cf5 (\cf9 "Model's Bias:"\cf5 )\
    model.showBias()\
\
\
\
\
\cf4 def \cf12 doXor\cf5 ():\
    \cf11 print\cf5 ( \cf9 "--- XOR ---"\cf5 )\
    x = [[\cf10 1\cf4 ,\cf10 1\cf5 ]\cf4 ,\cf5 [\cf10 1\cf4 ,\cf10 0\cf5 ]\cf4 ,\cf5 [\cf10 0\cf4 ,\cf10 1\cf5 ]\cf4 , \cf5 [\cf10 0\cf4 ,\cf10 0\cf5 ]]\
    y = [[\cf10 0\cf5 ]\cf4 ,\cf5 [\cf10 1\cf5 ]\cf4 , \cf5 [\cf10 1\cf5 ]\cf4 , \cf5 [\cf10 0\cf5 ]]\
\
    model = NeuralNetwork(\cf13 neuronsNum\cf5 =[\cf10 2\cf4 , \cf10 2\cf4 , \cf10 1\cf5 ]\cf4 , \cf13 activationVector\cf5 =[\cf9 'sigmoid'\cf5 ]\cf4 , \cf13 lossFunction\cf5 =\cf9 "MSE"\cf4 ,\
                          \cf13 learningRate\cf5 =\cf10 .1\cf4 , \cf13 weights\cf5 =\cf4 None, \cf13 bias\cf5 =\cf4 None\cf5 )\
\
    \cf4 for \cf5 i \cf4 in \cf11 range\cf5 (\cf10 10000\cf5 ):\
        \cf4 for \cf5 index \cf4 in \cf11 range\cf5 (\cf11 len\cf5 (x)):\
            model.train(\cf13 input\cf5 =x[index]\cf4 ,\cf13 target\cf5 =y[index])\
\
    \cf11 print\cf5 (\cf9 "Predictions: "\cf5 )\
    \cf4 for \cf5 index2 \cf4 in \cf11 range\cf5 (\cf11 len\cf5 (x)):\
        \cf11 print\cf5 (\cf9 "\cf4 \\n\cf9 Predict: "\cf4 , \cf5 x[index2])\
        \cf11 print\cf5 (model.calculate(x[index2]))\
\
\
\
\
\
\cf4 def \cf12 main\cf5 ():\
    \cf6 program_name \cf5 = sys.argv[\cf10 0\cf5 ]\
    arguments = sys.argv[\cf10 1\cf5 :]\
    \cf11 print\cf5 (arguments)\
\
    \cf6 #Input validation:\
    \cf4 if \cf11 len\cf5 (arguments) != \cf10 1\cf5 :\
        \cf11 print\cf5 (\cf9 "Please, input only one of these: example, and, or xor"\cf5 )\
\
\
    input = [\cf9 "example"\cf4 , \cf9 "and"\cf4 , \cf9 "xor"\cf5 ]\
    userInput = input[\cf10 1\cf5 ]\
\
\
    \cf4 if \cf5 userInput \cf4 is \cf9 "example"\cf5 :\
        doExample()\
    \cf4 elif \cf5 userInput \cf4 is \cf9 "and"\cf5 :\
        doAnd()\
    \cf4 elif \cf5 userInput \cf4 is \cf9 "xor"\cf5 :\
        doXor()\
    \cf4 else\cf5 :\
        \cf6 #Input validation\
        \cf11 print\cf5 (\cf9 "Please, input: example, and, or xor"\cf5 )\
        \cf4 return \cf10 0\
\
\
\
    \cf9 '''\
    #Random weights:\
    Newweights = None\
    model = NeuralNetwork(inputLen=2, layersNum = 2, neuronsNum = [3,2], activationVector = 0, lossFunction = "MSE", learningRate = .1, weights = Newweights)\
    model.showWeights()\
    out= model.calculate([1,0])\
    '''\
\
\
\cf4 if \cf5 __name__ == \cf9 "__main__"\cf5 :\
    main()\
\
}